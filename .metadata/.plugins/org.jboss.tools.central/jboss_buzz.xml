<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Using Dekorate to generate Kubernetes manifests for Java applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/28qOSk7b5HQ/" /><category term="DevOps" /><category term="Java" /><category term="Kubernetes" /><category term="Microservices" /><category term="Dekorate" /><category term="deployment" /><category term="kubernetes manifest" /><category term="openshift" /><author><name>Aurea Munoz Hernandez</name></author><id>https://developers.redhat.com/blog/?p=861907</id><updated>2021-03-17T07:00:09Z</updated><published>2021-03-17T07:00:09Z</published><content type="html">&lt;p&gt;To deploy an application on &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; or &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, you first need to create &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/"&gt;objects&lt;/a&gt; to allow the platform to install an application from a container image. Then, you need to launch the application using a &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;pod&lt;/a&gt; and expose it as a service with a static IP address. Doing all of that can be tedious, but there are ways to simplify the process.&lt;/p&gt; &lt;p&gt;Kubernetes follows a &lt;em&gt;declarative model&lt;/em&gt;, meaning that the user declares the desired application state and the cluster adjusts to match. Developers use files called &lt;em&gt;manifests&lt;/em&gt; to describe the desired state. Manifests are typically defined in YAML or JSON files, which are communicated to the server through its &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/"&gt;REST API endpoint&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Object formats are complex, with many fields to manipulate. It&amp;#8217;s a good idea to use a tool to help with creating the manifests. If you’re deploying &lt;a target="_blank" rel="nofollow" href="/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications, consider using &lt;a target="_blank" rel="nofollow" href="http://dekorate.io"&gt;Dekorate&lt;/a&gt;. Not only will it simplify your work as a developer, but it will also flatten your learning curve as you adopt Kubernetes.&lt;/p&gt; &lt;p&gt;In this article, we&amp;#8217;ll use Dekorate to generate Kubernetes and OpenShift manifests for a generic Java application. Our example is a simple REST API application.&lt;/p&gt; &lt;h2&gt;Making Java projects easier with Dekorate&lt;/h2&gt; &lt;p&gt;The Dekorate project offers a collection of Java annotations and processors that automatically update Kubernetes manifests during your application’s compilation. Developers can customize the manifests using either annotations or properties in configuration files, without the hassle of editing individual XML, YAML, or JSON templates.&lt;/p&gt; &lt;p&gt;Each annotation field maps to a property, so anything you specify with Java annotations can also be specified with properties, like so: &lt;code&gt;dekorate.[simplified-annotation-name].[&lt;a target="_blank" rel="nofollow" href="https://medium.com/better-programming/string-case-styles-camel-pascal-snake-and-kebab-case-981407998841"&gt;kebab-cased-property-name&lt;/a&gt;]&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The simplified name corresponds to the annotation’s class name, set as lowercase and stripped of suffixes (for example, &lt;code&gt;application&lt;/code&gt;). You can find a complete reference of the supported properties &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate/blob/master/assets/config.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Dekorate works with practically any Java build tool, including Maven, Gradle, Bazel, and sbt. To make life even easier, Dekorate also detects Java frameworks such as &lt;a target="_blank" rel="nofollow" href="/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="/topics/quarkus/"&gt;Quarkus&lt;/a&gt;, and Thorntail, aligning the generated manifests accordingly. See the article &lt;a target="_blank" rel="nofollow" href="/blog/2019/08/15/how-to-use-dekorate-to-create-kubernetes-manifests/"&gt;&lt;em&gt;How to use Dekorate to create Kubernetes manifests&lt;/em&gt; for more details&lt;/a&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate/blob/master/examples"&gt;Try out these examples&lt;/a&gt; if you want to play with Dekorate and discover the power of this fantastic tool for yourself.&lt;/p&gt; &lt;h2&gt;A simple REST application in Java&lt;/h2&gt; &lt;p&gt;Dekorate was designed to work with several Java frameworks, but it works just as well with plain Java. In the next sections, we&amp;#8217;ll use Dekorate to generate Kubernetes and OpenShift manifests for a generic Java application. Our example is a simple REST API. You can &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless"&gt;find the complete source code on GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Deployment objects&lt;/h3&gt; &lt;p&gt;There are three Kubernetes objects Dekorate must generate to deploy an application. Each of these objects—and their OpenShift platform equivalents—plays a different role, as described in Table 1.&lt;/p&gt; &lt;table align="”center”"&gt; &lt;caption&gt;&lt;strong&gt;Table 1: Objects used to deploy applications on Kubernetes and OpenShift.&lt;/strong&gt;&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Kubernetes resource&lt;/th&gt; &lt;th&gt;OpenShift resource&lt;/th&gt; &lt;th&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.6/applications/deployments/what-deployments-are.html#deployments-and-deploymentconfigs_what-deployments-are"&gt;DeploymentConfig&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Deploy and update applications as a pod.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;Service&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/core_concepts/pods_and_services.html#services"&gt;Service&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Provide an endpoint to access the pod as a service.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/networking/routes.html"&gt;Route&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Expose the service to clients outside of the cluster.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Next, we&amp;#8217;ll review the steps for creating manifests with Dekorate so that they include the resources listed in Table 1.&lt;/p&gt; &lt;h3&gt;Configuring the manifests&lt;/h3&gt; &lt;p&gt;The easiest way to enable Dekorate is to add the corresponding JAR file to the classpath using a Maven dependency:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62;     &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62;     &amp;#60;artifactId&amp;#62;kubernetes-annotations&amp;#60;/artifactId&amp;#62;      &amp;#60;version&amp;#62;0.13.6&amp;#60;/version&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: To keep your projects up to date, make sure you use &lt;a href="https://github.com/dekorateio/dekorate/releases"&gt;the latest version of Dekorate&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We need to specify that we would like to use Dekorate during the compilation phase of the Maven build life cycle. We can do this by adding the &lt;code&gt;@Dekorate&lt;/code&gt; annotation to our main Java class. Here, we&amp;#8217;ll use the &lt;code&gt;@KubernetesApplication&lt;/code&gt; annotation, which provides Kubernetes-specific configuration options. Edit the &lt;code&gt;App.java&lt;/code&gt; class and add the annotation:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication( . . .       name = "hello-world-fwless-k8s",        ... )&lt;/pre&gt; &lt;p&gt;This configuration creates a &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt; during the Maven compile goal. Dekorate uses the &lt;code&gt;name&lt;/code&gt; parameter value to specify the resource name and populate the corresponding Kubernetes label (&lt;code&gt;app.kubernetes.io/name&lt;/code&gt;). The &lt;code&gt;.yml&lt;/code&gt; and &lt;code&gt;.json&lt;/code&gt; manifest files are created in the &lt;code&gt;target/classes/META-INF/dekorate/*.{json,yml}&lt;/code&gt; folder.&lt;/p&gt; &lt;p&gt;As I mentioned in the introduction, Dekorate has a lightweight integration with build automation systems like Maven and Gradle. As a result, it can read the information from the tool configuration without bringing the build tool itself into the classpath. In our example, Dekorate would have used the name &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/dc58a3358f1e6618ff9b743803c521d8411bfacc/pom.xml#L5"&gt;maven artifactId&lt;/a&gt; by default, but we overrode it with the &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/dekorate-4-k8s/src/main/java/org/acme/App.java#L21"&gt;annotation name parameter&lt;/a&gt;. Dekorate will add this name as &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/"&gt;Kubernetes labels&lt;/a&gt; to all resources, including images, containers, deployments, and services.&lt;/p&gt; &lt;h3&gt;The annotation port parameter&lt;/h3&gt; &lt;p&gt;The next step is to add the &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/cea00f8199f8c2bf04fb431b0766a701a5f6ce62/src/main/java/org/acme/App.java#L22"&gt;annotation port parameter&lt;/a&gt;. This lets us configure the &lt;code&gt;Service&lt;/code&gt; endpoint and specify how it should be mapped with the Java HTTP port:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...     ports = @Port(name = "web", containerPort = 8080),     ... )&lt;/pre&gt; &lt;p&gt;Configuring the annotation port parameter adds the container port within the &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt; resource. That also generates a &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;Service&lt;/a&gt; resource that allows access to the endpoint inside the Kubernetes cluster. Once again, if you use a framework like Spring Boot or Quarkus, you can bypass the port definition. Dekorate will use the existing &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate#framework-integration"&gt;framework metadata&lt;/a&gt; to generate the resources.&lt;/p&gt; &lt;h3&gt;The Ingress resource&lt;/h3&gt; &lt;p&gt;Having a service or endpoint available internally within the cluster is great. But to simplify your life, we will now configure an additional &lt;code&gt;Ingress&lt;/code&gt; resource to make the service visible from your laptop. Both resources configure the proxy application (the Ingress controller) on the cluster to redirect external traffic and internal applications.&lt;/p&gt; &lt;p&gt;Add the following &lt;code&gt;expose&lt;/code&gt; parameter to the Dekorate annotation:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...     expose = true,     ... )&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: Remember that if you modify the annotation parameters, you can verify that the change is reflected in the manifest by triggering the compilation and checking the value gathered in the manifest file.&lt;/p&gt; &lt;p&gt;Next, we&amp;#8217;ll add a &lt;code&gt;host&lt;/code&gt; parameter to specify the local IP address or DNS-resolvable hostname to access the service:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...      host = "fw-app.127.0.0.1.nip.io",    ... )&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;host&lt;/code&gt; determines the host under which the application will be exposed. The &lt;code&gt;Ingress&lt;/code&gt; &lt;a target="_blank" rel="nofollow" href="https://v1-16.docs.kubernetes.io/docs/concepts/architecture/controller/"&gt;controller&lt;/a&gt; running on the Kubernetes cluster uses the &lt;code&gt;host&lt;/code&gt; to add a new rule to forward the requests addressed to that host to the corresponding service (for example, &lt;code&gt;fw-app&lt;/code&gt;).&lt;/p&gt; &lt;h3&gt;The Deployment controller&lt;/h3&gt; &lt;p&gt;Finally, we need to tell the &lt;code&gt;Deployment&lt;/code&gt; controller how to behave when a new image is available on the container registry. Setting the &lt;code&gt;ImagePullPolicy&lt;/code&gt; parameter to &lt;code&gt;Always&lt;/code&gt; ensures the controller will redeploy a new application as soon as a new container image is available:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...      imagePullPolicy = ImagePullPolicy.Always,     ... )&lt;/pre&gt; &lt;h2&gt;Deploying the application&lt;/h2&gt; &lt;p&gt;To sum up, here&amp;#8217;s the Dekorate configuration using the Java annotation parameters we&amp;#8217;ve discussed:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     name = "hello-world-fwless-k8s",  ports = @Port(name = "web", containerPort = 8080),      expose = true, host = "fw-app.127.0.0.1.nip.io", imagePullPolicy = ImagePullPolicy.Always ) &lt;/pre&gt; &lt;p&gt;The generated manifests will appear in the &lt;code&gt;target/classes/META-INF/dekorate&lt;/code&gt; folder as &lt;code&gt;kubernetes.yml&lt;/code&gt; and &lt;code&gt;kubernetes.json&lt;/code&gt; files.&lt;/p&gt; &lt;p&gt;Alternatively, you can deploy the application on OpenShift using the OpenShift-specific dependency. Edit the &lt;code&gt;pom.xml&lt;/code&gt; and add the following:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;openshift-annotations&amp;#60;/artifactId&amp;#62; &amp;#60;version&amp;#62;0.13.6&amp;#60;/version&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p&gt;Now, edit the &lt;code&gt;App.java&lt;/code&gt; class and add the &lt;code&gt;@OpenShiftApplication&lt;/code&gt; annotation:&lt;/p&gt; &lt;pre&gt;@OpenshiftApplication(     name = "hello-world-fwless-openshift",  ports = @Port(name = "web", containerPort = 8080),      expose = true, imagePullPolicy = ImagePullPolicy.Always )&lt;/pre&gt; &lt;p&gt;This time, the manifests are named &lt;code&gt;openshift.yml&lt;/code&gt; and &lt;code&gt;openshift.json&lt;/code&gt;, and they will contain the OpenShift resources listed in Table 1: &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.6/applications/deployments/what-deployments-are.html#deployments-and-deploymentconfigs_what-deployments-are"&gt;DeploymentConfig&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/core_concepts/pods_and_services.html#services"&gt;Service&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/networking/routes.html"&gt;Route&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Configuring the image build strategy&lt;/h2&gt; &lt;p&gt;Running applications on Kubernetes and OpenShift requires that we package them into a container image that the pod will bootstrap at launch time. The process of packaging the source code within a container image, container registry, or image name is what we call the &lt;em&gt;image build strategy&lt;/em&gt;. Dekorate currently supports several modes, depending on whether you use Kubernetes or OpenShift.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s see how to configure the image build strategy with Dekorate.&lt;/p&gt; &lt;h3&gt;Docker (Kubernetes and OpenShift)&lt;/h3&gt; &lt;p&gt;To configure the Docker image build strategy with Dekorate to use the Docker client as a build tool locally, add the following dependency to the &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62;  &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;docker-annotations&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p&gt;If no registry is specified, Docker.io will be used by default. Recompile the project using Maven or Gradle. The generated manifest file will include the container image as part of the &lt;code&gt;Deployment&lt;/code&gt; or &lt;code&gt;DeploymentConfig&lt;/code&gt; resource:&lt;/p&gt; &lt;pre&gt;image: amunozhe/hello-world-fwless:1.0-SNAPSHOT&lt;/pre&gt; &lt;p&gt;These parameters will be used when you launch the tool responsible for building the container image. We&amp;#8217;ll examine this in more detail &lt;a href="#creating_and_sharing"&gt;later&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to use an alternative image registry, edit the &lt;code&gt;App.java&lt;/code&gt; class to add the &lt;code&gt;@DockerBuild&lt;/code&gt; annotation to specify the &lt;code&gt;registry&lt;/code&gt; where the client will push the image:&lt;/p&gt; &lt;pre&gt;@DockerBuild(registry = "quay.io")&lt;/pre&gt; &lt;h3&gt;Source-to-Image (S2I) (OpenShift only)&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/builds/understanding-image-builds.html#build-strategy-s2i_understanding-image-builds"&gt;Source-to-Image (S2I)&lt;/a&gt; is a tool for building Docker-formatted images on OpenShift. With S2I, an OpenShift controller performs the image build, so you don’t have to build the container image locally.&lt;/p&gt; &lt;p&gt;Until recently, Dekorate only supported S2I for image builds; the S2I resource &lt;code&gt;BuildConfig&lt;/code&gt; is generated by Dekorate out of the box when the &lt;code&gt;@OpenShiftApplication&lt;/code&gt; annotation is used. To bypass generating the &lt;code&gt;BuildConfig&lt;/code&gt; resource, you can use the &lt;code&gt;@S2iBuild(enabled=false)&lt;/code&gt; parameter.&lt;/p&gt; &lt;h2&gt;Generating the manifests&lt;/h2&gt; &lt;p&gt;Once we&amp;#8217;ve configured the Kubernetes or OpenShift manifests, we can execute a &lt;code&gt;mvn package&lt;/code&gt; command to generate the actual manifests. As already mentioned, two files will be created in the &lt;code&gt;target/classes/META-INF/dekorate&lt;/code&gt; directory: &lt;code&gt;kubernetes.json&lt;/code&gt; and &lt;code&gt;kubernetes.yml&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now, we are ready to build the container image according to the specified image build strategy.&lt;/p&gt; &lt;h2 id="creating_and_sharing"&gt;Creating and sharing the container image&lt;/h2&gt; &lt;p&gt;Nowadays, many technologies exist to build container images locally. This example introduces two of them: &lt;a target="_blank" rel="nofollow" href="https://docs.docker.com/engine/reference/commandline/image_build/"&gt;Docker&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://github.com/GoogleContainerTools/jib"&gt;Jib&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Docker&lt;/h3&gt; &lt;p&gt;A Dockerfile is a text file used to build an image with Docker or &lt;a target="_blank" rel="nofollow" href="https://podman.io/"&gt;Podman&lt;/a&gt;. It contains instructions for containerizing your Java application. A &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/dekorate-4-k8s/Dockerfile"&gt;Dockerfile&lt;/a&gt; is provided with the example project&amp;#8217;s source code and is available in the project&amp;#8217;s root directory.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Dekorate does not generate Dockerfiles. It also doesn’t provide internal support for performing image builds and pushes.&lt;/p&gt; &lt;p&gt;If you need to install the Docker client locally, you can &lt;a target="_blank" rel="nofollow" href="https://docs.docker.com/get-docker/"&gt;download it here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Run the following command to build the image:&lt;/p&gt; &lt;pre&gt;docker build -f Dockerfile -t amunozhe/hello-world-fwless:1.0-SNAPSHOT .&lt;/pre&gt; &lt;p&gt;This image is only available within the Docker registry on your laptop. We need to push the image to an external image registry to allow Kubernetes or OpenShift to run it. For this example, we&amp;#8217;ll push the image to Docker Hub using my Docker Hub ID (amunozhe). You can register your own ID on the &lt;a target="_blank" rel="nofollow" href="http://hub.docker.com"&gt;Docker Hub website&lt;/a&gt; or use another publicly-available registry like &lt;a target="_blank" rel="nofollow" href="https://quay.io/"&gt;Red Hat Quay&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Once you’re logged in to the registry, you can finally push the image to make it available for the cluster:&lt;/p&gt; &lt;pre&gt;docker push amunozhe/hello-world-fwless:1.0-SNAPSHOT&lt;/pre&gt; &lt;h4&gt;Build and push the image via Dekorate&lt;/h4&gt; &lt;p&gt;Instead of manually executing the commands to build and push the Java container image, you can delegate the task to Dekorate, which uses &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate#docker-build-hook"&gt;hooks&lt;/a&gt; to support such features. This means you can perform all of the necessary steps with a single command:&lt;/p&gt; &lt;pre&gt;mvn clean package -Ddekorate.build=true  -Ddekorate.push=true&lt;/pre&gt; &lt;h3&gt;Jib&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin"&gt;Jib&lt;/a&gt; simplifies the process of creating container images by letting you avoid writing a Dockerfile. You don’t even need to have a Docker client installed to create and publish container images with Jib. Using Jib (via a Maven plugin) is also nice because it catches any changes made to the application every time you build.&lt;/p&gt; &lt;p&gt;To enable Jib, replace the &lt;code&gt;docker-annotations&lt;/code&gt; dependency with &lt;code&gt;jib-annotations&lt;/code&gt; in the &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;jib-annotations&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p&gt;We&amp;#8217;ll use the &lt;code&gt;@JibBuild&lt;/code&gt; annotation instead of &lt;code&gt;@DockerBuild&lt;/code&gt; to configure the registry. Edit the &lt;code&gt;application.java&lt;/code&gt; class to add the annotation:&lt;/p&gt; &lt;pre&gt;@JibBuild(registry = "docker.io")&lt;/pre&gt; &lt;p&gt;Run the following command to build and push the container image, taking advantage of the Dekorate hook:&lt;/p&gt; &lt;pre&gt;mvn clean package -Ddekorate.push=true&lt;/pre&gt; &lt;h2&gt;Deploying the application&lt;/h2&gt; &lt;p&gt;Once the Kubernetes and OpenShift manifests are generated, and the image is pushed to an image registry, we can deploy the application on the cluster under the demo &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"&gt;namespace&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To create your microservice on a cloud platform, you must have access to an active Kubernetes or OpenShift cluster. Here is the command to execute for Kubernetes:&lt;/p&gt; &lt;pre&gt;kubectl create ns demo kubectl apply -f target/classes/META-INF/dekorate/kubernetes.yml -n demo&lt;/pre&gt; &lt;p&gt;And here&amp;#8217;s the one for OpenShift:&lt;/p&gt; &lt;pre&gt;oc new-project demo oc apply -f target/classes/META-INF/dekorate/openshift.yml &lt;/pre&gt; &lt;p&gt;Wait a few moments until the pod is created and accessible through the external URL registered as an &lt;code&gt;Ingress&lt;/code&gt; or &lt;code&gt;Route&lt;/code&gt; resource. You can verify that the application is ready to accept the request by checking to see if the pod status is &lt;code&gt;RUNNING&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Kubernetes users should use this command:&lt;/p&gt; &lt;pre&gt;kubectl get pods -n demo&lt;/pre&gt; &lt;p&gt;For OpenShift users, it is:&lt;/p&gt; &lt;pre&gt;oc project demo oc get pods&lt;/pre&gt; &lt;p&gt;Next, you need to get the URL to access the application in the browser. Use the following command for Kubernetes:&lt;/p&gt; &lt;pre&gt;kubectl get ingress -n demo NAME                     CLASS HOSTS                     ADDRESS     PORTS AGE hello-world-fwless-k8s   &amp;#60;none&amp;#62;  fw-app.127.0.0.1.nip.io    localhost   80      147m&lt;/pre&gt; &lt;p&gt;As you can see in the &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/master/src/main/java/org/acme/App.java#L14"&gt;main Application class&lt;/a&gt;, the &lt;code&gt;/api/hello&lt;/code&gt; path provides the endpoint. To check if the application is accessible, open your browser and go to &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="http://fw-app.127.0.0.1.nip.io/api/hello"&gt;http://fw-app.127.0.0.1.nip.io/api/hello&lt;/a&gt;&lt;/code&gt;. You should see the following message:&lt;/p&gt; &lt;pre&gt;Hello From k8s FrameworkLess world!&lt;/pre&gt; &lt;p&gt;Here is how to check the application with OpenShift:&lt;/p&gt; &lt;pre&gt;oc get route -n demo NAME               HOST/PORT                             PATH  PORT hello-world-fwless-openshift  hello-world-fwless-openshift-demo.88.99.12.170.nip.io   /        8080&lt;/pre&gt; &lt;p&gt;In the browser, go to &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="http://hello-world-fwless-openshift-demo.88.99.12.170.nip.io/api/hello"&gt;http://hello-world-fwless-openshift-demo.88.99.12.170.nip.io/api/hello&lt;/a&gt;&lt;/code&gt;. You should see the following message:&lt;/p&gt; &lt;pre&gt;Hello from OpenShift FrameworkLess world!&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;As a developer, I love creating new features, improving user experiences, and discovering ways to deploy applications quickly and painlessly. As you&amp;#8217;ve seen in this article, Dekorate makes writing Kubernetes or OpenShift manifests practically effortless.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#038;title=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" data-a2a-url="https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/" data-a2a-title="Using Dekorate to generate Kubernetes manifests for Java applications"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/"&gt;Using Dekorate to generate Kubernetes manifests for Java applications&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/28qOSk7b5HQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;To deploy an application on Kubernetes or Red Hat OpenShift, you first need to create objects to allow the platform to install an application from a container image. Then, you need to launch the application using a pod and expose it as a service with a static IP address. Doing all of that can be tedious, [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/"&gt;Using Dekorate to generate Kubernetes manifests for Java applications&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">861907</post-id><dc:creator>Aurea Munoz Hernandez</dc:creator><dc:date>2021-03-17T07:00:09Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/</feedburner:origLink></entry><entry><title type="html">Time series component for Dashbuilder</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2_WfA1oWiwQ/time-series-component-for-dashbuilder.html" /><author><name>Manaswini Das</name></author><id>https://blog.kie.org/2021/03/time-series-component-for-dashbuilder.html</id><updated>2021-03-16T18:37:38Z</updated><content type="html">As you probably already know, you can use, a part of Business Central to create pages and intuitive dashboards. In Dashbuilder, Pages are composed of small components that can show any type of data. Dashbuilder provides by default multiple components that users can drag to pages. Recently, we have added a bunch of new components like treemaps, charts, maps, etc, to extend the usability and aid in representing user data in a precise way. Recently, we added the Prometheus dataset provider to extend the usability of Dashbuilder to represent time series metrics (see more details on this ). In this blog post, I’m going to walk you through the new external component added to Dashbuilder to better represent time-series data and use it to create your own dashboards connected to your time series datasets. TIME SERIES COMPONENT This is one of the new components that we have added using React and library. Now, you can provide a custom dataset or Prometheus metrics and create visualizations of your time series data on a line or area chart using Dashbuilder. ApexCharts is an MIT licensed open-source library to create interactive JavaScript charts built on SVG. You can find it on. ApexCharts provides some in-built features like downloading the dataset in CSV or downloading the chart in PNG or SVG format. You just have to click the sandwich menu icon on the top right corner to discover it. The component that I have used is zoomable time series, which means you can choose to zoom in and out a particular area of the chart. To use the time series component on Dashbuilder, get the component from and paste it in the Components directory. For more information about Developing custom components, head over to . After adding the component to the aforementioned directory and enabling external components, just click on the External Components dropdown, and select time-series-chart and drag it to the page, select the dataset in the Data tab(make sure that the columns are selected properly), set the component properties in the Component Editor tab and you are done. In order to add datasets, click on the menu dropdown in the navigation bar, select Datasets. You will be asked the type of dataset you want to add. You can add a CSV dataset or Prometheus metrics according to your choice. Based on what you name the datasets while adding them, the names of datasets in the Data tab will be populated after dragging the component to the page. You can use any library of your choice to create a component, just add the library, for instance, react-apexcharts and apexcharts to package.json and import them in respective TypeScript or JavaScript files. Configure the data to the required format to feed it to the component, for example, the major props that the Zoomable series chart uses includes options and series. The Options interface, which takes care of the x-axis categories and the chart name and the Series interface, which takes the arrays of names and the series values, looks like this: View the code on . We have the following component properties to make it customizable: * Show Area: A checkbox to set the type of chart, area, or line; * Chart Name: To set the chart name; * Date Categories: A checkbox to handle categories as dates or pure text; * Labels: To enable or disable data labels on data points; * Transposed: Whether the dataset provided uses series as separate columns or as rows. Time series component in action CONCLUSION There is no limit to what library/framework you want to use. We are continuing to include more custom components to allow users to create interactive dashboards. Huge shoutout to for coming up with integrating the Prometheus dataset provider with Dashbuilder and for the chart GIF. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2_WfA1oWiwQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Manaswini Das</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/time-series-component-for-dashbuilder.html</feedburner:origLink></entry><entry><title type="html">Apache Camel 3.9 - No more saw tooth JVM garbage collection</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/S2XnFEGUmug/apache-camel-39-no-more-saw-tooth-jvm.html" /><author><name>Claus Ibsen</name></author><id>http://feedproxy.google.com/~r/ApacheCamel/~3/D69y7h3uZWA/apache-camel-39-no-more-saw-tooth-jvm.html</id><updated>2021-03-16T13:07:00Z</updated><content type="html">We continue our effort to optimize Apache Camel. This is blog post part 7 which covers are latest effort on dramatically reducing the object allocations caused by Camel while routing messages. The good news is that we have overachieved and was able to reduce object allocations to ZERO!!! - so no more JVM memory usage graphs with saw tooth (note: in real world use-cases there will always be user data causing object allocations - but I wanted to have a click-bait blog title). To help identify potential areas of improvement in the core Camel, we put together a , which has only a single route triggered by a timer every producing 1000 msg/sec. These messages are routed to 10 different log endpoints (logging turned off). This allows us to focus on the internals of Camel only and what code paths is executed and what objects are being allocated and in-use by the internal routing engine. There are no message data (body or headers), or network communication etc.  Running the example (JVM heap size set to max 32mb) for 10 minutes profiled by JFR and browsed in JDK mission control we can see the dramatic difference.  In Camel 3.8 597mb of objects is allocated by Camel in total. And in Camel 3.9 that is ZERO. How did we get to zero? That is a long journey that started about a year ago, and we have gradually optimised Camel which I have blogged about in the 6 parts preceding this post. All this work is like pealing an onion, layer after layer. As one layer has been optimised, then the profiler reveals another layer, and so on. This time we could identify 5 areas for improvements: * consumers * core EIP patterns * internal routing processor * error handler * exchange and message The consumers are the source of incoming messages into Apache Camel. And so that is a great place to start. It's the consumers that allocate a new exchange, populate the exchange with message data such as body and headers.  After that it's the internal routing engine that routes the exchange via EIP patterns. And here we identified several spots where we could eliminate object allocations, or reduce allocations when some features are not in use etc. Error handling is one of the most complex part in the core Camel, and it uses objects to keep state in case of exceptions to handle redeliveries and whatnot. We were able to split the error handling into two tasks that operate either as a simplified or complex task. In the core EIP patterns we were able to optimize code that reduces object allocations. The 5th area we optimized is the exchange object. EIPs and the Camel routing engine store state per exchange on the exchange instance itself as exchange properties. That data is stored in a Map which means for each entry both a key is allocated in the java.util.Map. We optimized this to use an internal object array where each key is hardcoded as an index entry in the array. That means read/write is very fast and simple as its just an array index.  And then we ..... cheated ... instead of allocating new objects (via new constructor) we recycle existing objects from the previous exchange to the next. In other words we are using a sort of object pooling - this feature is called in Camel. Exchange Pooling The diagram above with ZERO object allocation is in fact with exchange pooling enabled. If exchange pooling is turned off (default), then the diagram should have been as below: As you can see there is saw-tooth graph. However the total object allocation is gone down from 597mb to 492mb (18% reduction). Awesome this is fantastic. And yes indeed it is. However when using anything there are both pros and cons, and so with object pooling. There is tiny tiny overhead of Camel to manage the object pools, and to "scrub" objects before they can be recused. That is a possibly a very very tiny CPU overhead compared to the JVM allocate and initialise new objects; instead of pool reuse. The biggest con is object leaks .. if objects are no returned back in the pool. Therefore you can turn on statistics which will report a WARN if a leak is detected when you stop Camel. The objects must be manually returned back into the pool, which we have coded in all the Camel components, and of course in core Camel. Now object leaks in this situation is not severe as you just have a situation as if there are no pooling, the JVM will create a new object - so the object allocations goes up, but its not severe like a database pool leaking TCP network connections. Upcoming work There are a few very complex EIP patterns and Camel component which does not yet support object pooling. We have this on the roadmap for Camel 3.10. Camel 3.9 is planned for release in March 2021.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/S2XnFEGUmug" height="1" width="1" alt=""/&gt;</content><dc:creator>Claus Ibsen</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/D69y7h3uZWA/apache-camel-39-no-more-saw-tooth-jvm.html</feedburner:origLink></entry><entry><title>Three ways to containerize .NET applications on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hKLJqjbCuFA/" /><category term=".NET Core" /><category term="Containers" /><category term="Kubernetes" /><category term="Linux" /><category term="Windows" /><category term=".NET OpenShift" /><category term="Linux containers" /><category term="OpenShift CNV" /><category term="Windows containers" /><author><name>Don Schenck</name></author><id>https://developers.redhat.com/blog/?p=855247</id><updated>2021-03-16T07:00:05Z</updated><published>2021-03-16T07:00:05Z</published><content type="html">&lt;p&gt;When Microsoft &lt;a target="_blank" rel="nofollow" href="https://devblogs.microsoft.com/dotnet/net-core-is-open-source/"&gt;announced in November 2014&lt;/a&gt; that the &lt;a target="_blank" rel="nofollow" href="/topics/dotnet"&gt;.NET Framework&lt;/a&gt; would be &lt;a target="_blank" rel="nofollow" href="/topics/open-source/"&gt;open source&lt;/a&gt;, the .NET developer&amp;#8217;s world shifted. This was not a slight drift in a new direction; it was a tectonic movement with huge implications.&lt;/p&gt; &lt;p&gt;For one thing, it positioned .NET developers to take part in the rapidly-growing world of Linux &lt;a target="_blank" rel="nofollow" href="/topics/containers/"&gt;containers&lt;/a&gt;. As container development matured to include technologies such as &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="/topics/serverless-architecture"&gt;Knative&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, .NET developers faced the opportunity—and challenge—of moving existing applications to use &lt;a target="_blank" rel="nofollow" href="/topics/microservices/"&gt;microservices&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/topics/containers"&gt;containers&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Fortunately, we have options. This article describes three paths for .NET development for OpenShift: Linux containers, Windows containers, and OpenShift container-native virtualization.&lt;/p&gt; &lt;h2&gt;Linux is containers, containers are Linux&lt;/h2&gt; &lt;p&gt;.NET Core is the most obvious and direct route to .NET in containers. Containerizing a .NET Core application is as simple as writing a Dockerfile and building the image using Podman or Docker. While it does require either a Linux-based computer or the proper Windows-based environment (Hyper-V or Windows Subsystem for Linux 2), no special steps or OpenShift technologies are needed. Build the image and import it to OpenShift.&lt;/p&gt; &lt;p&gt;Using &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; containers would seem to be the preferred route for new (&amp;#8220;greenfield&amp;#8221;) applications because no porting or rewriting is needed. If you want to port an existing application, two tools can help: Microsoft&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/Microsoft/dotnet-apiport"&gt;ApiPort&lt;/a&gt; and Amazon&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://aws.amazon.com/porting-assistant-dotnet/"&gt;Porting Assistant for .NET&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to start writing .NET applications to run in containers while keeping your existing monolith running in Windows, I suggest learning about the &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer-demos/cloud-native-compass/blob/master/strangler-pattern.md"&gt;strangler pattern&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to start a new application, or if you can easily port your current application, or if you want to employ the strangler pattern, .NET Core running Linux containers is the way to go.&lt;/p&gt; &lt;p&gt;But what if you want to move an existing .NET Framework application immediately?&lt;/p&gt; &lt;h2&gt;Windows containers&lt;/h2&gt; &lt;p&gt;The long-awaited &lt;a href="https://developers.redhat.com/blog/category/windows/"&gt;Windows&lt;/a&gt; containers technology is here and ready for you to run with it on OpenShift. Using the same management model—and the same management plane—as Linux containers, Windows containers allow you to build and run images on the Windows operating system.&lt;/p&gt; &lt;p&gt;What is the practical takeaway of this? How about the ability to run .NET Framework applications in containers? Yes, &lt;em&gt;Framework&lt;/em&gt;. Not Core; Framework. Let your imagination run wild with that for a moment. Build a Windows container image, and running your IIS website can be as easy as &lt;code&gt;docker run -d -p 80:80 my-iis-website&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;There are, of course, a few things to consider when using this approach. I will explore these points in an upcoming article about Windows containers on OpenShift. Stay tuned.&lt;/p&gt; &lt;h2&gt;OpenShift CNV&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/topics/containers/what-is-container-native-virtualization"&gt;OpenShift container-native virtualization&lt;/a&gt;, or CNV, is built on the &lt;a target="_blank" rel="nofollow" href="https://kubevirt.io/"&gt;KubeVirt&lt;/a&gt; technology. The KubeVirt project lets you control a virtual machine (VM) with Kubernetes, treating it like a container. The many advantages of this technology, coupled with OpenShift, include immediate lift-and-shift without any code changes, role-based access control (RBAC) and networking policies as afforded by OpenShift, and more. In short, you get all the advantages of Kubernetes and OpenShift while keeping your Windows VM running.&lt;/p&gt; &lt;p&gt;If you want to move your legacy .NET Framework applications to OpenShift and benefit from its features &lt;em&gt;immediately&lt;/em&gt;, this route could be the best. It buys you time as you evaluate your path forward, which might include containers—Windows, Linux, or both.&lt;/p&gt; &lt;h2&gt;More to come&lt;/h2&gt; &lt;p&gt;This article was only a brief introduction to the latest technologies for containerizing .NET applications. Your individual path depends on your situation, budget, time, and so on. To help make the decision easier, look for three more articles on this topic—one for each path: Linux containers, Windows containers, and OpenShift CNV with a Windows VM.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#038;title=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/" data-a2a-title="Three ways to containerize .NET applications on Red Hat OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/"&gt;Three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hKLJqjbCuFA" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;When Microsoft announced in November 2014 that the .NET Framework would be open source, the .NET developer&amp;#8217;s world shifted. This was not a slight drift in a new direction; it was a tectonic movement with huge implications. For one thing, it positioned .NET developers to take part in the rapidly-growing world of Linux containers. As [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/"&gt;Three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">855247</post-id><dc:creator>Don Schenck</dc:creator><dc:date>2021-03-16T07:00:05Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/</feedburner:origLink></entry><entry><title type="html">Supply chain integration - Example store integration architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VbDHkj9ufWE/supply-chain-integration-example-store-integration-architecture.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/57bT5V1qM3Y/supply-chain-integration-example-store-integration-architecture.html</id><updated>2021-03-16T06:00:00Z</updated><content type="html">Part 3 - Example store integration architecture  In  from this series shared a look at the logical common architectural elements found in supply chain integration for retail stores. The process was laid out how I've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  It started with laying out the process of how I've approached the use case by researching successful customer portfolio solutions as the basis for a generic architectural blueprint. Having completed our discussions on the logical view of the blueprint, it's now time to look at a specific example. This article walks you through an example store integration scenario showing how expanding the previously discussed elements provides a blueprint for your own store integration scenarios. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's my intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but I've chosen a format that I hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this blueprint and outline the solution. STORE INTEGRATION ARCHITECTURE The story behind store integration of their supply chain is that you have many distinct actors involved, as you see starting on the right side of the diagram above. There are suppliers, order managers, warehouse management staff, and any number of third-party supply chain systems that need to have access for providing data to and from the supply chain systems.  Using API management regulates the authorisation and authentication before access is granted to the integration framework. Note the use of event streams which is an indicator that there is an attempt to provide quick processing and action around events in the supply chain lifecycle. This allows for up to the minute reporting on data, supply volumes, and other interesting data for the retail organisation. Message transformation is a core need when connecting systems and data messages through an integration backend, as not all messages are going to map easily from one system to another, or from one microservice to another. Using a transformation service allows for a consistent data model for the organisation regardless of the destination end point needs.  The core integration framework is found in the supply chain microservices, a collection of microservice integrations that manages all the needs of the actors directly providing or requesting information around the retail supply chain. Supporting access to the Retail Data Framework, a separate and detailed architecture blueprint to be covered in another series, you'll find integration data microservices specifically helping with data access and processing. Along the same lines you have integration microservices tying together all external systems such as third-party supply chain systems and any eventual use of an AI / Machine Learning platform. The core to supply chain integration remains a solid fundamental need for cloud-native integration technologies and ongoing maintenance structures in a retail organisation to support them. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the supply chain integration use case.  An overview of this series on the supply chain integration portfolio architecture blueprint can be found here: 1. 2. 3. Catch up on any articles you missed by following one of the links above. This completes the series and we hope you enjoyed this architecture blueprint for supply chain integration in retail. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VbDHkj9ufWE" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/57bT5V1qM3Y/supply-chain-integration-example-store-integration-architecture.html</feedburner:origLink></entry><entry><title type="html">A Genetic Algorithm with Trusty PMML</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w3YxeenNdDY/a-genetic-algorithm-with-trusty-pmml.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2021/03/a-genetic-algorithm-with-trusty-pmml.html</id><updated>2021-03-15T14:28:36Z</updated><content type="html">Recently, I’ve stumbled upon this interesting and paired about a Genetic Algorithm. Then, I’ve asked myself if somehow the features of could be meaningfully used inside such context. I won’t go deep into technical details, but basically, the Genetic Algorithm classifies features as "genes", a set of genes is a "genoma", and that represents a unit of evaluation. A population is made of multiple instances of such units. Each individual has a fitness value, that is based on the values of its genes, and the scope of the algorithm is to select the individual with the fittest value, or the fitness value equal to the expected one. During algorithm evaluation, new generations are created, where each new-born is the "combination" of two parents. The interesting feature is the "mutation" of such units, where some gene of one is exchanged with genes of another one. At each generation, fittest individuals replace older ones, and that will eventually lead to a "convergence", when the expected fitness value is obtained. I’m always fascinated by technologies that mimic a biological behavior to obtain their results, and Genetic Algorithm should somehow mimic the genetic evolution through sexual reproduction. But in real life the genetic evolution is due to two factors: 1. genetic recombination (specific to sexual reproduction) 2. spontaneous genetic mutation. In my implementation, I wanted to recreate both of them, and I wanted to demonstrate some cool features of Trusty PMML at the same time. To start with, I forked the original project and I begun to use PMML for gene representation. The next step has been to try to figure out concrete, likely example. During a party meeting, a colleague started to talk about plants and crops at home, and that gives me the idea. THE STORY OF GUS THE FARMER One day, Gus decided to leave his work to make an agriculture business. He did not have an extensive area to cultivate, so he decided to crop apartment plants inside greenhouses. He made a market survey to find out which plants were the most looked after, and he also gathered historical weather information regarding water and light available on the place he was going to build in. Being wise enough, he also devised a plan to give flexibility to his business, to maintain his profit even when customer taste or environment conditions would change. He finds out that people mostly like ferns, roses and cacti. He builds three greenhouses, each of which has a slightly different configuration, specialized for one kind of plant. He then started a setup phase, with five harvests a week for each greenhouse (those are very extraordinary greenhouses, actually), and then, at each harvest, he slightly tweaked the configuration of each one so that, in the end, all of them would produce the most required plant. He then successfully started his business and, whenever people prefer different plants or environment variables mutate, all he has to do is go through the setup phase again to remain in business. AND THE CODE ? This story is represented inside the Greenhouses branch of my project. It is a standard maven-project and is written in Kotlin; there is no claim of being mathematically or theoretically accurate, but it shows how some Trusty PMML features may be applied to a Genetic Algorithm. The setup phase is represented by the loop inside SimpleDemoGA class. Farm represents the overall installation, producing harvests at each iteration. Harvest is the product of greenhouses. And Greenhouse is the execution of a PMML model. The result of this execution may be fern, rose or cactus. At startup there are three slightly different PMML files: Greenhouse_A.pmml, Greenhouse_B.pmml and Greenhouse_C.pmml. Each of those PMMLs defines a Regression model with three RegressionTables, each of which targeting a specific plant: &lt;RegressionModel functionName="classification" modelName="..." targetFieldName="Species"&gt; &lt;RegressionTable targetCategory="fern" ..,&gt; ... &lt;/RegressionTable&gt; &lt;RegressionTable targetCategory="rose" ..,&gt; ... &lt;/RegressionTable&gt; &lt;RegressionTable targetCategory="cactus" ..,&gt; ... &lt;/RegressionTable&gt; &lt;/RegressionModel&gt; Each table has a different intercept value, and inside each of them a different coefficient is given to _water_ and _light_ variables &lt;RegressionTable targetCategory="..." intercept="1"&gt; &lt;NumericPredictor name="water" exponent="1" coefficient="..."/&gt; &lt;NumericPredictor name="light" exponent="1" coefficient="..."/&gt; &lt;/RegressionTable&gt; and that made every greenhouse specific for a given plant. When a greenhouse produce the required plant, its gene is set to "1", otherwise is "0". The fitness of a harvest is the number of "1" genes. When all of them are "1", it means that all greenhouses are producing the required plant, and the target is obtained. TRUSTY PMML IN ACTION The mutation of the greenhouse configuration mimics the natural genetic "casual" mutation, and to achieve that a cool feature of Trusty PMML has been exploited, i.e. the ability to change and quick reload the underlying model itself at runtime. This is not standard procedure. Usually, predictive models are created, trained and tuned with different tools, and PMML is generated out of them. This requires an overall company setup similar to the following one: 1. business analyst gather requirements 2. data scientists design and tune models 3. someone exports models in PMML format 4. developers write code that such PMML files (and a PMML engine) to actually return a value based on a given input. There may be cases, though, where this setup does not fulfill the business requirement. One example could be a daily incremental fine-tuning of an already generated model. Another requirement could be the usage of the PMML model slightly beyond its specification, and I have a concrete example of that. We received a request about the evaluation of the probability that a given string is similar to another one, both of which given as input by the user. This could be a particular use case of PMML defines an expression called that actually uses that distance to evaluate if one string is similar to another one. The problem is that the maxLevenshteinDistance is a fixed attribute, so TextIndex could be used to evaluate if two strings are similar for a specific Levenshtein distance, while the request could be translated somehow to "how many Levenshtein distances there are from those two strings ?". Of course it could be possible to write custom extension to solve that, and possibly even a really contrived PMML based on nested defined functions, replacement, regexp and what else. Modify that maxLevenshteinDistance attribute on-the-fly could be another solution. I know it would be practically unfeasible if the requests are too frequent, but this gives you an idea of a possible use case. BACK TO GUS At each harvest Gus needs to tweak the greenhouses. In algorithmic terms, a gene mutation has to happen. In the actual implementation, a modification of the PMML is required. This is done inside PMMLManager.mutatePMMLFile() method: 1. the last version of the PMML file is loaded as PMML instance 2. the intercept value of the RegressionTable for the required specie is increased 3. the modified PMML is serialized back as PMML file, overwriting the previous version 4. for each greenhouse of each harvest, that modified versions of PMML files are loaded and actual evaluation is done on top of that At the end of the cycle, when the target is obtained (i.e. an harvest with all three genes as "1") in the target/classes folder there will be the last versions of the PMML files, while the original ones are kept unmodified inside src/main/resources folder. SETUP IN ACTION The following is an exerpt of an actual execution of the program with the following input data Required specie: rose Environment values {water=48, light=5} During execution, the log will show the predicted values for each model while they are mutated: ... Model Greenhouse_C results {Predicted_Species=fern, Probability_rose=66.9, Species=fern, Probability_fern=91.30000000000001, Probability_cactus=-157.20000000000002} Model Greenhouse_A results {Predicted_Species=fern, Probability_rose=54.0, Species=fern, Probability_fern=65.5, Probability_cactus=-118.5} Model Greenhouse_B results {Predicted_Species=fern, Probability_rose=51.7, Species=fern, Probability_fern=56.900000000000006, Probability_cactus=-107.60000000000001} Model Greenhouse_C results {Predicted_Species=fern, Probability_rose=66.9, Species=fern, Probability_fern=91.30000000000001, Probability_cactus=-157.20000000000002} Model Greenhouse_A results {Predicted_Species=fern, Probability_rose=54.0, Species=fern, Probability_fern=65.5, Probability_cactus=-118.5} ... At each cycle, an overall report is printed out, with the genomas of all the five harvests: Population of 5 individual(s). Required specie: rose Environment values {water=48, light=5} Generation: 0 Fittest score: 0 ==Genetic Pool== &gt; Harvest 0 | [genes=[0, 0, 0]] | [features= Greenhouse_A: fern, Greenhouse_B: fern, Greenhouse_C: fern | &gt; Harvest 1 | [genes=[0, 0, 0]] | [features= Greenhouse_A: fern, Greenhouse_B: fern, Greenhouse_C: fern | &gt; Harvest 2 | [genes=[0, 0, 0]] | [features= Greenhouse_A: fern, Greenhouse_B: fern, Greenhouse_C: fern | &gt; Harvest 3 | [genes=[0, 0, 0]] | [features= Greenhouse_A: fern, Greenhouse_B: fern, Greenhouse_C: fern | &gt; Harvest 4 | [genes=[0, 0, 0]] | [features= Greenhouse_A: fern, Greenhouse_B: fern, Greenhouse_C: fern | ================ Progressively, some genes will switch from 0 to 1: Generation: 38 Fittest score: 2 ==Genetic Pool== &gt; Harvest 0 | [genes=[1, 1, 0]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: fern | &gt; Harvest 1 | [genes=[1, 1, 0]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: fern | &gt; Harvest 2 | [genes=[1, 1, 0]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: fern | &gt; Harvest 3 | [genes=[0, 1, 0]] | [features= Greenhouse_A: fern, Greenhouse_B: rose, Greenhouse_C: fern | &gt; Harvest 4 | [genes=[0, 1, 0]] | [features= Greenhouse_A: fern, Greenhouse_B: rose, Greenhouse_C: fern | ================ until the conclusion of the setup: Generation: 75 Fittest score: 3 ==Genetic Pool== &gt; Harvest 0 | [genes=[1, 1, 1]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: rose | &gt; Harvest 1 | [genes=[1, 1, 0]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: fern | &gt; Harvest 2 | [genes=[1, 1, 0]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: fern | &gt; Harvest 3 | [genes=[1, 1, 0]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: fern | &gt; Harvest 4 | [genes=[1, 1, 0]] | [features= Greenhouse_A: rose, Greenhouse_B: rose, Greenhouse_C: fern | ================ Solution found in generation 75 Fitness: 3 Genes: 111 Process finished with exit code 0 Last, as comparison, here’s a snipppet from the original Greenhouse_B.pmml &lt;RegressionTable targetCategory="fern" intercept="1"&gt; &lt;NumericPredictor name="water" exponent="1" coefficient="1.3"/&gt; &lt;NumericPredictor name="light" exponent="1" coefficient="-1.3"/&gt; &lt;/RegressionTable&gt; &lt;RegressionTable targetCategory="rose" intercept="2"&gt; &lt;NumericPredictor name="water" exponent="1" coefficient="0.9"/&gt; &lt;NumericPredictor name="light" exponent="1" coefficient="1.1"/&gt; &lt;/RegressionTable&gt; &lt;RegressionTable targetCategory="cactus" intercept="1"&gt; &lt;NumericPredictor name="water" exponent="1" coefficient="-1.3"/&gt; &lt;NumericPredictor name="light" exponent="1" coefficient="1.3"/&gt; &lt;/RegressionTable&gt; and here’s the mutated one after successful completion of setup: &lt;RegressionTable intercept="1" targetCategory="fern"&gt; &lt;NumericPredictor name="water" exponent="1" coefficient="1.3"/&gt; &lt;NumericPredictor name="light" exponent="1" coefficient="-1.3"/&gt; &lt;/RegressionTable&gt; &lt;RegressionTable intercept="29" targetCategory="rose"&gt; &lt;NumericPredictor name="water" exponent="1" coefficient="0.9"/&gt; &lt;NumericPredictor name="light" exponent="1" coefficient="1.1"/&gt; &lt;/RegressionTable&gt; &lt;RegressionTable intercept="1" targetCategory="cactus"&gt; &lt;NumericPredictor name="water" exponent="1" coefficient="-1.3"/&gt; &lt;NumericPredictor name="light" exponent="1" coefficient="1.3"/&gt; &lt;/RegressionTable&gt; CONCLUSION The code used in the example may be retrieved . The scope of this article and the companion project is to illustrate a possible usage of Trusty PMML for a Genetic Algorithm implementation. It has no claim at all about theoretical nor mathematical integrity. Any comment or suggestion is more than welcome. I hope you enjoyed it! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w3YxeenNdDY" height="1" width="1" alt=""/&gt;</content><dc:creator>Gabriele Cardosi</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/a-genetic-algorithm-with-trusty-pmml.html</feedburner:origLink></entry><entry><title>Planning your containerization strategy on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/a7PVx9lb3o4/" /><category term="CI/CD" /><category term="Containers" /><category term="Kubernetes" /><category term="containers" /><category term="Migration" /><category term="openshift" /><category term="scaling" /><author><name>John Tatman</name></author><id>https://developers.redhat.com/blog/?p=819487</id><updated>2021-03-15T07:00:52Z</updated><published>2021-03-15T07:00:52Z</published><content type="html">&lt;p&gt;There&amp;#8217;s no question about the benefits of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, including faster application delivery, resilience, and scalability. And with &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, there has never been a better time to take advantage of a cloud-native platform to containerize your applications.&lt;/p&gt; &lt;p&gt;Transforming your application delivery cycle and shifting from traditional infrastructure to cloud-native can be daunting, however. As with any path toward a solution, it helps to break down the process into segments to better understand how to get from point A to point B. This article provides a framework for approaching the conversation with your application team to ensure that your application is containerized and hosted on OpenShift as quickly and easily as possible.&lt;/p&gt; &lt;p&gt;Figure 1 is an overview of how to structure an effective containerization conversation. We&amp;#8217;ll go over each step in the next sections.&lt;/p&gt; &lt;div id="attachment_819507" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/11/Screen-Shot-2020-11-11-at-9.45.05-AM.png"&gt;&lt;img aria-describedby="caption-attachment-819507" class="wp-image-819507 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/11/Screen-Shot-2020-11-11-at-9.45.05-AM.png" alt="The diagram includes each part of the discussion described in the following sections." width="640" height="1060" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/11/Screen-Shot-2020-11-11-at-9.45.05-AM.png 640w, https://developers.redhat.com/blog/wp-content/uploads/2020/11/Screen-Shot-2020-11-11-at-9.45.05-AM-181x300.png 181w, https://developers.redhat.com/blog/wp-content/uploads/2020/11/Screen-Shot-2020-11-11-at-9.45.05-AM-618x1024.png 618w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-819507" class="wp-caption-text"&gt;Figure 1: Framing the containerization conversation.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Introductions: Start by establishing trust&lt;/h2&gt; &lt;p&gt;First impressions and friendliness can go a long way. Personally, I find that urging team members to ask questions when they feel unsure about a topic fosters a welcoming environment from the start. Summarizing your experience with the topic also builds credibility. Trust, positive reinforcement, and patience create the best setting for finding effective solutions, especially with a complicated subject like containerization.&lt;/p&gt; &lt;h2&gt;Review the tech stack&lt;/h2&gt; &lt;p&gt;This part of the conversation should revolve around a high-level description of what the application does, the language and middleware it uses, and other components that make up the application, such as databases and external services.&lt;/p&gt; &lt;p&gt;It is also important to note the application&amp;#8217;s overarching architecture: Are you working with a monolith that your team hopes to break down into a series of &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;, existing microservices that share common configuration patterns, or perhaps a series of services that should be separated because of non-dependencies?&lt;/p&gt; &lt;p&gt;The technical overview will dictate various aspects of the containerization strategy. It is important to cover every vital part of the application to determine which solution to pursue, but avoid spending too much time on the technical stack. You only need to know what technologies the application uses and how it runs; anything beyond that, and you risk discussing unnecessary facets of the application.&lt;/p&gt; &lt;h2&gt;Is your application containerized?&lt;/h2&gt; &lt;p&gt;Whether your application is containerized will likely become evident from the technical overview. If the answer is yes, you can jump right into discussing the application&amp;#8217;s state.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/topics/cloud-native-apps/stateful-vs-stateless"&gt;Application states&lt;/a&gt; refer to how application data is stored. &lt;em&gt;Stateful&lt;/em&gt; applications require persistent storage, whereas &lt;em&gt;stateless&lt;/em&gt; applications do not require storing server information or session data. There are benefits to both types of applications, and which architecture to use largely comes down to the application&amp;#8217;s purpose and how it functions. Luckily, containers are great for managing both stateful and stateless architectures. Talking to your team about each approach&amp;#8217;s costs and benefits is another opportunity to highlight the power of containers and how they can transform application delivery and maintenance.&lt;/p&gt; &lt;p&gt;If the application is not yet containerized, don&amp;#8217;t worry! Take this opportunity to introduce the concept of containers to your team and describe how using them could improve the application&amp;#8217;s deployment lifecycle and management on &lt;a href="https://developers.redhat.com/courses/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. Your team is likely somewhat familiar with containers and OpenShift. If not, a simple analogy can help explain how OpenShift Container Platform and containers work together.&lt;/p&gt; &lt;p&gt;Think of OpenShift Container Platform as a giant cargo ship with three main parts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;Shipping containers&lt;/b&gt;: Think of Docker containers as shipping containers. They&amp;#8217;re lightweight, standalone, executable software packages that include everything needed to run an application: source code, runtime, system tools, system libraries, and settings. A shipping container can contain a multitude of objects, just like a Docker container holds the various dependencies and source code an application needs to run.&lt;/li&gt; &lt;li&gt;&lt;b&gt;The crane&lt;/b&gt;: As we know, OpenShift Container Platform is built on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. In this analogy, Kubernetes acts as our crane, managing, scaling, and orchestrating containers on the cargo ship.&lt;/li&gt; &lt;li&gt;&lt;b&gt;The ship&lt;/b&gt;: The cargo ship itself is OpenShift. The ship holds the containers and the crane together and provides the added benefits of an interface, security, and developer tools for creating efficient workflows for containerized applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once the team has a high-level understanding of what containers are and how they are used with OpenShift, they can consider the containerization strategy. How to containerize is primarily based on the complexity of the application. If the application has many dependencies and requires many steps to build and run, it is best to use the Docker strategy. For simple, lightweight microservices, &lt;a href="https://developers.redhat.com/blog/tag/s2i/"&gt;Source-To-Image (S2I)&lt;/a&gt; is a good option.&lt;/p&gt; &lt;p&gt;When discussing the containerization strategy, asking questions will help you identify a specific containerization method. Questions such as &amp;#8220;What does the build process look like?&amp;#8221; and &amp;#8220;What scripts do we use for the application build process?&amp;#8221; provide greater insight into which containerization method is most applicable to the situation. You can even begin exploring the concept of &lt;a href="https://developers.redhat.com/blog/tag/ci-cd-pipeline/"&gt;continuous integration (CI) pipelines&lt;/a&gt; to add a layer of automation and security.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/resources/consulting-pathfinder-datasheet"&gt;Pathfinder&lt;/a&gt; may also be used to determine which applications can be containerized, the effort involved, and any impediments to containerization. Pathfinder can help you determine the order in which applications should migrate to containers based on factors such as business criticality, technical and execution risk, and the effort required.&lt;/p&gt; &lt;h2&gt;Application resource requirements&lt;/h2&gt; &lt;p&gt;Quite a few variables come into play when setting compute resources. The application&amp;#8217;s size and complexity will usually help determine how much CPU and memory your containers will need. It is also important to consider the resources allocated to the deployment environment. While your OpenShift cluster might work without initially setting resource requests and limits, you will encounter stability issues as the number of projects and teams on the cluster expands.&lt;/p&gt; &lt;p&gt;Tools such as resource quotas, which provide constraints for limiting resource consumption in a namespace, and limit ranges, which set resource usage limits for each type of resource, provide a foundation for successful resource requirement planning. You might perform preliminary tests to determine resource availability and requirements for each container and use this information as a basis for capacity planning and prediction. Remember, however, that resource usage analysis takes time, and your team will continue to fine-tune the application over its production lifecycle.&lt;/p&gt; &lt;h2&gt;Base images&lt;/h2&gt; &lt;p&gt;While there are many sources for base images, acquiring them from a known and trusted source can be challenging. It is important to use secure base images that are up to date and free of known vulnerabilities. If a vulnerability is discovered, you must update the base images. Luckily, you can use options such as &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.7/registry/architecture-component-imageregistry.html"&gt;Red Hat OpenShift Container Registry&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/technologies/cloud-computing/quay"&gt;Red Hat Quay&lt;/a&gt; to securely store and manage the base images for your applications.&lt;/p&gt; &lt;p&gt;OpenShift Container Registry runs on top of the existing cluster infrastructure and provides an out-of-the-box solution for users to manage the images that run their workloads. The added benefit of locally-managed images with OpenShift Container Registry is the speed at which developers can use stored images to quickly start their applications. Quay, on the other hand, is an enterprise-quality container image registry with advanced registry features, including geo-replication, image scanning, and the ability to roll-back images.&lt;/p&gt; &lt;p&gt;Using these tools as examples of how to safely deploy and store base images will help highlight the security that OpenShift provides. You could also explore using the freely redistributable &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Image&lt;/a&gt; based on &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;Red Hat Enterprise Linux 8&lt;/a&gt;, a lightweight image with the benefits and security capabilities of enterprise &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Scaling with OpenShift Container Platform&lt;/h2&gt; &lt;p&gt;Many workloads have a dynamic nature that varies over time, making a fixed scaling configuration difficult to implement. Thankfully, you can use OpenShift&amp;#8217;s autoscaling feature to define varying application capacity that is not fixed but instead ensures just enough capacity to handle different loads.&lt;/p&gt; &lt;p&gt;When reviewing scaling with your team, it is important to note the many configurations that can occur—for example, horizontal scaling versus vertical scaling. Horizontal scaling is preferable to vertical scaling because it is less disruptive, especially for stateless services. That is not the case for stateful services, where you might prefer vertical scaling. Another scenario where vertical scaling is useful is when tuning a service&amp;#8217;s resource needs based on actual load patterns. There&amp;#8217;s also manual scaling versus automatic scaling—manual scaling should be viewed as an anticipatory solution to scaling with the circumstance of human interaction, whereas automated scaling is a more reactive approach.&lt;/p&gt; &lt;h2&gt;Allow time for Q &amp;#38; A&lt;/h2&gt; &lt;p&gt;The questions-and-answers (Q &amp;#38; A) portion of the conversation provides an opportunity to elaborate on any topics that could have otherwise detracted from the overall discussion flow. These topics are up to the discretion of the application team. Of course, your goal should be to ensure that the team feels comfortable with the concepts at hand. However, the purpose of your containerization discussion is to gather the necessary information to identify a suitable containerization strategy. Once a strategy is established, you can begin to teach the team in a more in-depth manner, ensuring that you efficiently use your valuable meeting time. For example, you could use the Q &amp;#38; A to explain the technical details of how S2I functions as a containerization strategy or how using &lt;a href="https://developers.redhat.com/blog/2020/09/07/keeping-kubernetes-secrets-secret/"&gt;secrets&lt;/a&gt; adds an additional layer of security to your workflow.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;When exploring containerization with your application team, it is important to recognize that containers are not always the most beneficial method. For teams that are already practicing good containerization methods and are interested in more of a cloud-native approach, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; might be a better alternative. Teams that can configure their environments easily, with a small resource footprint for application deployment, might not need to use containers, to begin with.&lt;/p&gt; &lt;p&gt;Containers are very powerful for improving an application&amp;#8217;s time-to-market delivery. When used alongside OpenShift, you can easily manage the maintenance, scaling, and lifecycle of your containers. As your team becomes more familiar with container foundations and OpenShift, consider &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous integration/continuous delivery&lt;/a&gt; (CI/CD) tools such as &lt;a href="https://developers.redhat.com/blog/2019/07/19/getting-started-with-tekton-on-red-hat-openshift/"&gt;Tekton&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/tag/argocd/"&gt;ArgoCD&lt;/a&gt; to automate and improve the application delivery cycle.&lt;/p&gt; &lt;p&gt;Different variables are at play in every development environment. A well-constructed conversation about the utility of containers will help you identify the best application deployment approach for your team.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#38;linkname=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#38;linkname=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#38;linkname=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#38;linkname=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#38;linkname=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#38;linkname=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#38;linkname=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F15%2Fplanning-your-containerization-strategy-on-red-hat-openshift%2F&amp;#038;title=Planning%20your%20containerization%20strategy%20on%20Red%20Hat%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2021/03/15/planning-your-containerization-strategy-on-red-hat-openshift/" data-a2a-title="Planning your containerization strategy on Red Hat OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/15/planning-your-containerization-strategy-on-red-hat-openshift/"&gt;Planning your containerization strategy on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/a7PVx9lb3o4" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;There&amp;#8217;s no question about the benefits of containers, including faster application delivery, resilience, and scalability. And with Red Hat OpenShift, there has never been a better time to take advantage of a cloud-native platform to containerize your applications. Transforming your application delivery cycle and shifting from traditional infrastructure to cloud-native can be daunting, however. As [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/15/planning-your-containerization-strategy-on-red-hat-openshift/"&gt;Planning your containerization strategy on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/15/planning-your-containerization-strategy-on-red-hat-openshift/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">819487</post-id><dc:creator>John Tatman</dc:creator><dc:date>2021-03-15T07:00:52Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/15/planning-your-containerization-strategy-on-red-hat-openshift/</feedburner:origLink></entry><entry><title type="html">Monitoring Quarkus apps using Micrometer and Prometheus into OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FGgano-hBrw/" /><author><name /></author><id>https://quarkus.io/blog/micrometer-prometheus-openshift/</id><updated>2021-03-15T00:00:00Z</updated><content type="html">Metrics are the measurements of any aspect of an application such as resource usage or behaviors. We can expose these metrics from our Quarkus applications by using the Micrometer extension via the /q/metrics endpoint. What metrics will be exposed? Only by adding the Micrometer extension, a lot of metrics are...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FGgano-hBrw" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/micrometer-prometheus-openshift/</feedburner:origLink></entry><entry><title type="html">Building Prometheus Dashboards in Business Central</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/G7LBy7BAHUw/building-prometheus-dashboards-in-business-central.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2021/03/building-prometheus-dashboards-in-business-central.html</id><updated>2021-03-12T19:21:30Z</updated><content type="html">Dashbuilder is the project part of the Business Central suite responsible for dashboards and data sets. It can read data from multiple types of data set sources, including CSV, SQL, ElasticSearch, and Kie Server or you can create your own source of data using Java programming language. In jBPM 7.50.0 Final, we introduced a new type of provider for data sets: Prometheus. WHY PROMETHEUS Prometheus is the facto standard for collecting metrics. It has connectors to very well-known systems, such as Kafka and metrics can be easily consumed from third-party systems. Furthermore, Kie Server by default also exports ! Kie Server Prometheus Metrics PROMETHEUS IN BUSINESS CENTRAL To access Prometheus metrics from Business Central you must create a Data Set using Data Sets editor: Admin -&gt; Data Sets -&gt; New Data Set. Data Sets providers in Business Central Select Prometheus on the list and then provide the Prometheus installation in the form. The field “Query” accepts any , but pay attention to the fact that if a query does not result from data then no error is thrown, just an empty data set will be created Creating a Prometheus Data Set The result will contain at least the columns TIME and VALUE, then other columns will be created according to the result “metric” object. Sample Data Set data preview The above result was parsed from the following JSON: Prometheus Query Response SAMPLE DASHBOARD Let’s create some dashboards for the metrics exposed in Kie Server focusing on process instances metrics. The dashboard we will create can be filtered by container id and constantly pulls data from the server. Dashboard build using data coming from Prometheus To build our dashboard we need to do the following steps: 1. Create Prometheus data sets 2. Create the dashboard 3. Configure the dashboard for filtering and data refresh Let’s take a deep dive on those! STEP 1: CREATING PROMETHEUS DATA SETS We need to create 4 data sets. Notice that we recommend that you have some data available on Prometheus when creating the data sets, otherwise not all columns will be available. * Process Instances: Completed — The total number of processes completed. It uses the metrics kie_server_process_instance_completed_total * Process Instances: Completed Recently — a sliding time window that shows processes finished in the last 10 minutes, slicing by every 5 seconds. The query is: kie_server_process_instance_completed_total[10m:5s] * Process Instances: Completed — The total number of processes started. It uses the metric kie_server_process_instance_started_total * Process Instances: Started Recently — a sliding time window that shows processes started in the last 10 minutes, slicing by every 5 seconds. The query is: kie_server_process_instance_started_total[10m:5s] Now that we have data sets in place it is time to build the dashboard. STEP 2: BUILDING THE DASHBOARD Dashboards can be created in Business Central by selecting Pages from the main menu, check our about dashboards or the . Our simple dashboard contains a combo box to select containers, two bar charts, and two external components: Adding the container filter: In the page editor find Reporting group and drag a Filter to the page. Select combo box and use the data set Process Instances: Started using the container_id for the entries. Adding a Combo Box to filter the dashboard Adding bar charts: The data set for the bar chart uses the column value from Prometheus for the Y values and the process definition ID for the X-axis (categories). This step should be done for Process Instances: Started and Process Instances: Completed datasets. Creating Bar Charts to summarize the totals Adding time series: Time Series is a great created by my colleague . It has the power of transposing a data set column to be used as series. When transposing the data set, the time series can use the second data set column to build the series for the chart: Time Series component This step should be done for Process Instances: Started Recently and Process Instances: Completed Recently data sets. You may add static HTML elements to increase the dashboard, but from this point we are ready to start the configuration. Dashboard structure in Page Editor STEP 3: CONFIGURING FILTERING AND DATA REFRESH The final step to build our dashboard is to set up data refresh (polling) and filtering. Data Refresh: All the components have refresh turned on so it can poll data from Prometheus each X seconds. Select the component configuration by clicking on three dots in the upper right side of the component. Go to the Display tab and configure refresh on Refresh section. This must be done for all components that will be automatically refreshed. Data Refresh configuration Filtering: All components that will receive the filter must also have the Listen to Others flag on the Filter section selected. This way it will have the data set filtered when we select a container using the combo box we added earlier. Filtering configuration TESTING THE DASHBOARD Let’s test the dashboard by creating some data. We suggested using the Evaluation and Mortgage Process examples projects, but you can also create your own. This is our dashboard with data coming from Prometheus metrics: Dashboard in action with some data We can also filter the data to see the process from a specific container. Filtering by Container Now, your Dashbuilder dashboards based on Prometheus are ready for use! Now you can: * Add the dashboard to the Business Central so other uses can access it; * Export the Dashboard to run on . CONCLUSION In this post, we discussed the new Prometheus Data Set support in Business Central. In the next post let’s create a new dashboard for the Business Central tasks! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/G7LBy7BAHUw" height="1" width="1" alt=""/&gt;</content><dc:creator>William Siqueira</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/building-prometheus-dashboards-in-business-central.html</feedburner:origLink></entry><entry><title>No more Java in vscode-xml 0.15.0!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/k1Sl-O2U6HY/" /><category term="Java" /><category term="Linux" /><category term="Mac" /><category term="VS Code" /><category term="Windows" /><category term="GraalVM" /><category term="vscode-xml" /><category term="XML extension for VS Code" /><author><name>David Thompson</name></author><id>https://developers.redhat.com/blog/?p=867757</id><updated>2021-03-12T08:00:43Z</updated><published>2021-03-12T08:00:43Z</published><content type="html">&lt;p&gt;Among other improvements and bug fixes in the &lt;code&gt;vscode-xml&lt;/code&gt; extension 0.15.0 release, you can now run the extension without needing &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. We know the Java requirement &lt;a target="_blank" rel="nofollow" href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-xml&amp;#38;ssr=false#review-details"&gt;discouraged many people from trying the extension&lt;/a&gt;. We have included a new setting, &lt;b&gt;Prefer Binary&lt;/b&gt; (&lt;code&gt;xml.server.preferBinary&lt;/code&gt;) that lets you choose between the Java server and the new binary server. We&amp;#8217;re excited to remove the Java restriction from &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;Red Hat’s XML extension for Visual Studio Code&lt;/a&gt; in &lt;code&gt;vscode-xml&lt;/code&gt; 0.15.0. Keep reading to find out how we did it.&lt;/p&gt; &lt;h2&gt;LemMinX, Java, and vscode-xml&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/lemminx"&gt;Eclipse LemMinX&lt;/a&gt; is the language server that provides &lt;code&gt;vscode-xml&lt;/code&gt;&amp;#8216;s XML editing features. By creating a language server, we can provide XML editing capabilities not only to Visual Studio Code (&lt;a href="https://developers.redhat.com/blog/category/vs-code/"&gt;VS Code&lt;/a&gt;) but also to other text editors, such as Sublime, Eclipse IDE, Emacs, and Vim.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: To learn more about language servers and the Language Server Protocol (LSP), please see &lt;a href="https://developers.redhat.com/blog/2016/06/27/a-common-interface-for-building-developer-tools/"&gt;&lt;i&gt;A common interface for building developer tools&lt;/i&gt;&lt;/a&gt;, which does a great job of explaining the topic.&lt;/p&gt; &lt;p&gt;LemMinX was written in Java. Using Java to code LemMinX allows for using existing XML libraries. For instance, LemMinX uses the Xerces library to validate XML files against schemas. Until the &lt;code&gt;vscode-xml&lt;/code&gt; 0.15.0 release, we required a Java Runtime Environment (JRE), because we distributed LemMinX as a JAR file that had to be interpreted by the Java runtime. LemMinX also supports a number of useful extensions, either as individual JAR files or as a ZIP file containing multiple JARs. Current extensions include &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/lemminx-maven"&gt;lemminx-maven &lt;/a&gt; (for Maven &lt;code&gt;pom.xml&lt;/code&gt; files), &lt;a target="_blank" rel="nofollow" href="https://github.com/Treehopper/liquibase-lsp"&gt;liquibase-lsp&lt;/a&gt; (for liquibase XML files), and &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/liberty-language-server/tree/master/lemminx-liberty"&gt;lemminx-liberty&lt;/a&gt; (for OpenLiberty &lt;code&gt;server.xml&lt;/code&gt; files).&lt;/p&gt; &lt;p&gt;To allow the user to run &lt;code&gt;vscode-xml&lt;/code&gt; without installing Java, we needed to rethink how we provided LemMinX functionality to the user.&lt;/p&gt; &lt;h2&gt;GraalVM native-image&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.graalvm.org/reference-manual/native-image/"&gt;GraalVM native-image&lt;/a&gt; is a tool that can compile a Java program into a standalone, platform-specific executable binary. Unlike other tools, it doesn’t only package a copy of Java along with the program. GraalVM compiles the Java bytecode into native instructions, and it includes a small library that manages memory. When we use GraalVM, the generated binary doesn&amp;#8217;t include all the code that normally goes into a Java installation. The executable binary in LemMinX is even smaller than &lt;a target="_blank" rel="nofollow" href="https://download.eclipse.org/justj/jres/11/downloads/latest/"&gt;these minimal Java installations&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 1 shows the sequence by which GraalVM native-image creates an executable. The Java compiler transforms Java source code into Java bytecode. Then, GraalVM native-image turns the Java bytecode into native instructions.&lt;/p&gt; &lt;div id="attachment_867787" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram.png"&gt;&lt;img aria-describedby="caption-attachment-867787" class="wp-image-867787" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram.png" alt="The steps to transform Java source code into a native binary." width="640" height="88" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram.png 780w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram-300x41.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram-768x105.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-867787" class="wp-caption-text"&gt;Figure 1: How GraalVM native-image transforms Java bytecode into native instructions.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;GraalVM native-image has limitations. The binary that GraalVM produces works only for a specific operating system and CPU architecture. As a result, we need to compile and distribute separate binaries for Windows, macOS, and &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;. The macOS binary is compiled for the x86_64 CPU architecture but also works on Apple Silicon through the &lt;a target="_blank" rel="nofollow" href="https://developer.apple.com/documentation/apple_silicon/about_the_rosetta_translation_environment"&gt;Rosetta 2 translation layer&lt;/a&gt;. Another limitation is that all the code that you want to run must be present during the native-image process. &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/vscode-xml/blob/master/docs/Extensions.md#custom-xml-extensions"&gt;Extensions to the base XML editing functionality&lt;/a&gt;, such as &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/lemminx-maven"&gt;lemminx-maven&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://github.com/Treehopper/liquibase-lsp"&gt;liquibase-lsp&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/liberty-language-server/tree/master/lemminx-liberty"&gt;lemminx-liberty&lt;/a&gt;, won&amp;#8217;t work when using the binary server.&lt;/p&gt; &lt;h2&gt;Delivering the binaries&lt;/h2&gt; &lt;p&gt;We do not package the binaries into &lt;code&gt;vscode-xml&lt;/code&gt;. Instead, when &lt;code&gt;vscode-xml&lt;/code&gt; starts up and detects that you don’t have Java installed, it downloads the binary specific to your computer’s operating system. This workaround reduces the total amount of data downloaded to make &lt;code&gt;vscode-xml&lt;/code&gt; work. The extension comes with the SHA 256 checksums of the binaries so that it can check the integrity of the downloaded binary.&lt;/p&gt; &lt;h2&gt;When will vscode-xml use a binary?&lt;/h2&gt; &lt;p&gt;You can use the flowchart in Figure 2 to figure out whether &lt;code&gt;vscode-xml&lt;/code&gt; will download and run a binary. Note that you can enable the &lt;b&gt;Prefer Binary&lt;/b&gt; setting (&lt;code&gt;xml.server.preferBinary&lt;/code&gt;) if you have a Java runtime but still want VS Code to use the binary.&lt;/p&gt; &lt;div id="attachment_867797" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart.png"&gt;&lt;img aria-describedby="caption-attachment-867797" class="wp-image-867797" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart.png" alt="The steps required in sequence to determine whether a binary or Java server should be started." width="640" height="328" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart.png 1003w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart-300x154.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart-768x394.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-867797" class="wp-caption-text"&gt;Figure 2: How vscode-xml determines whether to start a binary or a Java server.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If Java is not installed, &lt;code&gt;vscode-xml&lt;/code&gt; launches the binary server. If it is installed and the &lt;b&gt;Prefer Binary&lt;/b&gt; setting is &lt;em&gt;disabled&lt;/em&gt;, &lt;code&gt;vscode-xml&lt;/code&gt; launches the Java server. If it is installed and the &lt;b&gt;Prefer Binary&lt;/b&gt; setting is &lt;em&gt;enabled&lt;/em&gt;, &lt;code&gt;vscode-xml&lt;/code&gt; launches the Java server if LemMinX extensions are detected; otherwise, it launches the binary server.&lt;/p&gt; &lt;p&gt;The LemMinX server extensions shown in Figure 2 are extensions onto &lt;code&gt;vscode-xml&lt;/code&gt;’s functionality, as I described earlier. If the extensions are present along with the Java runtime, they override the &lt;b&gt;Prefer Binary&lt;/b&gt; setting because these extensions won’t work with the binary server.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article offered an inside look at how we removed the Java requirement in &lt;code&gt;vscode-xml&lt;/code&gt;. Thanks to everybody who contributed to this release! To see a list of all the changes and bug fixes, please see the &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/vscode-xml/blob/master/CHANGELOG.md"&gt;vscode-xml 0.15.0 changelog&lt;/a&gt;. If you encounter bugs or have ideas for improving &lt;code&gt;vscode-xml&lt;/code&gt;, we hope you will &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/vscode-xml/issues/new/choose"&gt;submit an issue&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#038;title=No%20more%20Java%20in%20vscode-xml%200.15.0%21" data-a2a-url="https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/" data-a2a-title="No more Java in vscode-xml 0.15.0!"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/"&gt;No more Java in vscode-xml 0.15.0!&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/k1Sl-O2U6HY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Among other improvements and bug fixes in the vscode-xml extension 0.15.0 release, you can now run the extension without needing Java. We know the Java requirement discouraged many people from trying the extension. We have included a new setting, Prefer Binary (xml.server.preferBinary) that lets you choose between the Java server and the new binary server. [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/"&gt;No more Java in vscode-xml 0.15.0!&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">867757</post-id><dc:creator>David Thompson</dc:creator><dc:date>2021-03-12T08:00:43Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/</feedburner:origLink></entry></feed>
